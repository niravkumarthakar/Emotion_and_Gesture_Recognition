# -*- coding: utf-8 -*-
"""Final mini-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RlwFsYJaKQiEI45X-A4uuuyv38yrMaR3

# **Mini-Project**
"""

import tensorflow as tf
import numpy as np
import cv2
import ipywidgets as widgets
from IPython.display import display, Image as IPyImage, clear_output
from google.colab import files
from PIL import Image
import io

# Define your trained model paths
model_paths = {
    "Emotion Detection": "/content/emotion_detection_model.h5",
    "Hand Gesture Detection": "/content/hand_gesture_model.h5"
}

# Class labels
emotion_labels = ["Angry", "Disgust", "Fear", "Happy", "Neutral", "Sad", "Surprise"]
gesture_labels = sorted(["Fine", "Fist", "No", "OK", "Pointing", "Victory"])

# UI elements
model_dropdown = widgets.Dropdown(
    options=model_paths.keys(),
    description="Model:",
)

upload_button = widgets.FileUpload(
    accept="image/*", multiple=False
)

display(model_dropdown)
display(upload_button)

# Load the selected model
def load_model(model_name):
    model_path = model_paths.get(model_name)
    if model_path:
        try:
            model = tf.keras.models.load_model(model_path, compile=False)
            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
            return model
        except Exception as e:
            print(f"Error loading model: {e}")
    return None

# Preprocess image based on the model
def preprocess_image(image_data, model_name):
    img = Image.open(io.BytesIO(image_data)).convert("RGB")

    if model_name == "Emotion Detection":
        img = img.convert("L")
        img = img.resize((48, 48))
    else:
        img = img.convert("L")
        img = img.resize((64, 64))

    img_array = np.array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=-1)
    img_array = np.expand_dims(img_array, axis=0)
    return img_array

# Predict and show result
def predict(change):
    clear_output(wait=True)
    display(model_dropdown, upload_button)

    if not upload_button.value:
        print("Please upload an image!")
        return

    uploaded_file = next(iter(upload_button.value.values()))
    image_data = uploaded_file["content"]

    # Show selected image
    print("Uploaded Image:")
    display(IPyImage(data=image_data))

    selected_model_name = model_dropdown.value
    model = load_model(selected_model_name)

    if model is None:
        print("Error: Model not loaded correctly!")
        return

    img_array = preprocess_image(image_data, selected_model_name)
    prediction = model.predict(img_array)

    predicted_class = np.argmax(prediction)

    # Check if predicted_class is within the valid range for labels
    labels = emotion_labels if selected_model_name == "Emotion Detection" else gesture_labels

    if predicted_class >= len(labels):
        print(f"Error: Predicted class index ({predicted_class}) is out of range for {selected_model_name} labels.")
        return

    confidence = prediction[0][predicted_class] * 100
    result = labels[predicted_class]

    print(f"\nPredicted {selected_model_name}: {result} ({confidence:.2f}% confidence)")

upload_button.observe(predict, names="value")