# -*- coding: utf-8 -*-
"""Gesture_Emotion_Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pGuZ7tc0VTYDdBXuMGQyFK8REIHwqJdv

#  Gesture and Emotion Detection using CNN
This notebook demonstrates how to load pre-trained models for hand gesture and facial emotion recognition and use them for prediction on test images.

# Install & Import Required Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
# %matplotlib inline

"""# Load Models"""

gesture_model = load_model('models/gesture_model.h5')
emotion_model = load_model('models/emotion_model.h5')

"""# Class Labels"""

gesture_labels = ['fist', 'palm', 'ok', 'victory', 'pointing', 'no', 'fine']
emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

"""# Gesture Image Prediction"""

img = cv2.imread('sample_gesture.jpg', cv2.IMREAD_GRAYSCALE)
img = cv2.resize(img, (64, 64))
img = img.reshape(1, 64, 64, 1) / 255.0

prediction = gesture_model.predict(img)
predicted_gesture = gesture_labels[np.argmax(prediction)]

plt.imshow(img.squeeze(), cmap='gray')
plt.title(f"Predicted Gesture: {predicted_gesture}")
plt.axis('off')
plt.show()

"""# Emotion Image Prediction"""

img = cv2.imread('sample_emotion.jpg', cv2.IMREAD_GRAYSCALE)
img = cv2.resize(img, (48, 48))
img = img.reshape(1, 48, 48, 1) / 255.0

prediction = emotion_model.predict(img)
predicted_emotion = emotion_labels[np.argmax(prediction)]

plt.imshow(img.squeeze(), cmap='gray')
plt.title(f"Predicted Emotion: {predicted_emotion}")
plt.axis('off')
plt.show()

"""# Conclusion

- Both gesture and emotion recognition models work well with grayscale image inputs.
- You can extend this notebook to real-time webcam prediction using OpenCV.
- Customize with more test samples or your own trained models.

"""